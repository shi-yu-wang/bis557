---
title: "Homework 5"
author: "Shiyu Wang"
output: pdf_document
  html_document:
    df_print: paged
Imports: glmnet,Matrix
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The linear model vignette}
-->

This homework is due by the end of the day on November 5th 2018. Solutions should appear as a vignette in your package called "homework-3". If you do not want to write your solution for questions 2 - 4 in \LaTeX, you may provide a solution using paper and pencil and include an image in your write-up. 

1. CASL page 117, question 7.
```{r}
# Set seeds
set.seed(666)

# Calculate the kernel
kern_epan<-function(x){
  ran <- as.numeric(abs(x) <= 1)
  k<-3/4*(1-x^2)*ran
  return(k)
}

# Write kernel density
kern_density<-function(x,h,x_new){
  dst<-c()
  for (i in 1:length(x_new)) {
    dst[i]<-mean(kern_epan((x-x_new[i])/h))/h
  }
  dst
}

# Visualize results
x<-rnorm(1000, 0, 1)
x_new<-sort(rnorm(100, 0, 1))
h<-c(0.1,0.2,0.5,1,2,5)
for (i in 1:length(h)) {
  plot(x=x_new,y=kern_density(x,h[i],x_new),xlab = "Hand-constructed Dataset",ylab = "Kernel Density Estimation", main = paste("Kernel Density Estimation for the Hand-constructed Dataset When Bandwidth=", as.character(h[i])))
}
```

2. CASL page 200, question 3
soppuse f and g are convex functions, now we are prooving function $\Gamma=f+g$ is also a convex function.
Since f and g are convex, thus, we have: $f(\theta x +(1-\theta) y) \le \theta f(x) + (1-\theta)f(y)$. $\Gamma(\theta x+(1-\theta)y)=f(\theta x +(1-\theta) y) + g(f\theta x +(1-\theta) y) \le \theta f(x) + (1-\theta)f(y) + \theta g(x) + (1-\theta)g(y) = \theta \Gamma (x) + (1-\theta)\Gamma(y)$\

3. CASL page 200, question 4
Soppose $f(x)=|x|$, Since $0\le\theta\le 1$, $f(\theta x +(1 - \theta)y) = |\theta x +(1 - \theta)y| \le \theta |x|+(1-\theta)|y|=\theta f(x) + (1-\theta)f(y)$. Thus, absolute value function is convex.\
$l_1$ norm $=\sum |x_i|$. Since each $|x_i|$ is convex, based on question 3, sum of them is still convex.\
4. CASL page 200, question 5
The elastic net objective function is the sum of $l_1$ norm and $l_2$ norm. Based on the above questions, we only have to show that $l_2$ norm is convex.\
Suppose $g(x)=x^2$, $g(\theta x+(1-\theta)y)=(\theta x+(1-\theta)y)^2=\theta^2 x^2 + 2\theta(1-\theta)xy+(1-\theta)^2y^2\le \theta^2 x^2 + (1-\theta)^2y^2 <= \theta g(x)+(1-\theta)g(y)$ (since $0\le\theta\le 1$). Thus, $g(x)$ is convex. Based on question 3, $l_2$ norm is convex, and from question 4, $l_1$ norm is convex, so that the sum of $l_1$ norm and $l_2$ norm is also convex.
5. CASL page 200, question 6
```{r}
# From textbook page 189:
# Check current KKT conditions for regression vector.
## Args:
# X: A numeric data matrix.
# y: Response vector.
# b: Current value of the regression vector.
# lambda: The penalty term.
##Returns:
# A logical vector indicating where the KKT conditions have
# been violated by the variables that are currently zero.
# KKT check function
kkt_check<-function(x, y, b, lambda){
  resids<-y-x %*% b
  s <- apply(x, 2, function(xj) crossprod(xj, resids))/lambda / nrow(x)
  # Return a vector indicating where the KKT conditions have
  # been violated by the variables that are currently zero.
  (b == 0) & (abs(s) >= 1)
}

# Employ iris as the data set for implementation (get hints from Wenting Gao for this bullet point)
x<-scale(model.matrix(Sepal.Length ~. -1, iris))
y<-iris[,1]
# Make the function
library(glmnet)
lasso_reg_with_screening <- function(x, y){
  # Take alpha as 1
  lasso_alpha<-cv.glmnet(x,y,alpha=1)
  # Use LSE to choose lambda
  lambda<-lasso_alpha$lambda.1se
  b<-lasso_alpha$glmnet.fit$beta[, lasso_alpha$lambda == lambda]
  kkt_check(x,y,b,lambda)
}
lasso_reg_with_screening(x, y)
```
