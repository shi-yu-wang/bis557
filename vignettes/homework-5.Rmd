---
title: "Homework 5"
author: "Shiyu Wang"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{HW5 vignette}
-->
# load required libraries
library(keras)
install_keras()
library(glmnet)
library(doMC)
registerDoMC()

# Question 1
# make training dataset and testing dataset
mnist<-dataset_mnist()
# training set
x_train<-mnist$train$x
dim(x_train)
y_train<-mnist$train$y
dim(y_train)
# testing set
x_test<-mnist$test$x
dim(x_test)
y_test<-mnist$test$y
dim(y_test)

# reshape the datasets to make the model
x_train<-array_reshape(x_train,dim=c(60000,28*28))
x_test<-array_reshape(x_test,dim=c(10000,28*28))

# implementation of the model
set.seed(666)
s<-sample(seq(1,length(y_train)),1000)
a<-data.matrix(x_train[s,])
b<-as.factor(y_train[s])
fit_1<-cv.glmnet(x=a,y=b,family="multinomial")

# out-of-sample error
pred_1<-predict(fit_1$glmnet.fit,data.matrix(x_test),s=fit_1$lambda.min,type = "class")
t_1<-table(as.vector(pred_1),y_test)
ose_1<-1-sum(diag(t_1))/sum(t_1)
ose_1

# Increase the out-of-sample prediction accuracy by extracting predictive features from the images
# decrease lambda by multiplying 0.6
pred_2<-predict(fit_1$glmnet.fit,data.matrix(x_test),s=fit_1$lambda.min*0.6,type = "class")
t_2<-table(as.vector(pred_2),y_test)
ose_2<-1-sum(diag(t_2))/sum(t_2)
ose_2
# the error is reduced from 0.1563 to 0.1561

# Question 2
# CASL Number 4 in Exercises 8.11. (Get hints from my friend Yingnan Lyu, credits to her!)
# Note that I will use the dataset from problem 1 since I can't load the dataset on my laptop. 
# (Regarding this problem I get help from my friend Yingnan Lyu, credits to her!)

# make test and training datasets
train_2<-data.frame(id=rep("train",60000),class=y_train)
test_2<-data.frame(id=rep("test",10000), class=y_test)
emnist <- rbind(train_2, test_2)
# rebuild x28
x28<-rbind(x_train, x_test)
dim(x28) <- c(70000,28,28,1)
# make test set
a<-t(apply(x28,1,cbind))
x_train_2<-a[emnist$id=="train",]
dim(x_train_2)<-c(60000, 28, 28, 1)
x_test_2 <- a[emnist$id!="train",]
dim(x_test_2)<-c(10000,28,28,1)
# make test set
y<-to_categorical(emnist$class, num_classes=26L)
y_train_2<-y[emnist$id == "train",]
y_test_2<-y[emnist$id!="train",]
# implement the model in textbook
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),input_shape = c(28, 28, 1),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2,2),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = "categorical_crossentropy",optimizer=optimizer_rmsprop(),metrics=c("accuracy"))
history <- model %>% fit(x_train_2, y_train_2, epochs = 10, validation_data = list(x_test_2, y_test_2))
model %>% keras::evaluate(x_test, y_test, verbose = 0)

# implement the model in the textbook while increasing the kernel size up to 3
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),input_shape = c(28, 28, 1),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")
model %>% compile(loss = "categorical_crossentropy",optimizer=optimizer_rmsprop(),metrics=c("accuracy"))
history <- model %>% fit(x_train_2, y_train_2, epochs = 10, validation_data = list(x_test_2, y_test_2))
model %>% keras::evaluate(x_test, y_test, verbose = 0)
# The running time of two models is extremely long so that I wasn't able to compare them.

# Question 3
# Based on the textbook, write the relaven functions:
# creat weights
casl_nn_make_weights<-function(sizes){
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)){
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),ncol = sizes[j],nrow=sizes[j+1L])
    weights[[j]] <- list(w=w,b=rnorm(sizes[j + 1L]))
    }
  weights
}

# apply a rectified linear unit (ReLU) to a vector/matrix.
casl_util_ReLU<-function(v){
  v[v < 0] <- 0
  v
}

# apply derivative of the rectified linear unit (ReLU).
casl_util_ReLU_p<-function(v){
  p <- v * 0
  p[v > 0] <- 1
  p
}

# derivative of the mean squared error (MSE) function.
casl_util_mse_p<-function(y, a){
  (a - y)
}

# apply forward propagation to a set of NN weights and biases.
casl_nn_forward_prop<-function(x, weights, sigma){
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)){
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
    }
  list(z=z, a=a)
}

# apply backward propagation algorithm.
casl_nn_backward_prop<-function(x, y, weights, f_obj, sigma_p, f_p){
  z <- f_obj$z; a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))){
    if (j == L){
      grad_z[[j]] <- f_p(y, a[[j]])
      } else {
        grad_z[[j]] <- (t(weights[[j + 1]]$w) %*%
                          grad_z[[j + 1]]) * sigma_p(z[[j]])
        }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
    }
  list(grad_z=grad_z, grad_w=grad_w)
}

# apply stochastic gradient descent (SGD) to estimate NN.
casl_nn_sgd <-function(X, y, sizes, epochs, eta, weights=NULL){
  if (is.null(weights)){
    weights <- casl_nn_make_weights(sizes)
    }
  for (epoch in seq_len(epochs)){
    for (i in seq_len(nrow(X))){
      f_obj <- casl_nn_forward_prop(X[i,], weights,casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,f_obj, casl_util_ReLU_p,casl_util_mse_p)
      for (j in seq_along(b_obj)){
        weights[[j]]$b <- weights[[j]]$b -eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -eta * b_obj$grad_w[[j]]
      }
    }
    }
  weights
}

# predict values from a training neural network.
casl_nn_predict <-function(weights, X_test){
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))){
    a <- casl_nn_forward_prop(X_test[i,], weights,casl_util_ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
    }
  y_hat
}

# employ mean absolute deviation (MAD) as a loss function
casl_util_mad_p<-function(y,a){
  (a-y)/abs(a-y)
}
casl_nn_sgd_mad <-function(X, y, sizes, epochs, eta, weights=NULL){
  if (is.null(weights)){
    weights <- casl_nn_make_weights(sizes)
    }
  for (epoch in seq_len(epochs)){
    for (i in seq_len(nrow(X))){
      f_obj <- casl_nn_forward_prop(X[i,], weights,casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,f_obj, casl_util_ReLU_p,casl_util_mad_p)
      for (j in seq_along(b_obj)){
        weights[[j]]$b <- weights[[j]]$b -eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w -eta * b_obj$grad_w[[j]]
      }
    }
    }
  weights
}

# simulation for 1000 times. here I refer to the way that my friend Wenfeng Zhang builds this dataset since I didn't come to the course for this homework and not sure the requirement for simulation. Credits to Wenfeng Zhang.
set.seed(666)
X<-matrix(runif(1000,min=-1,max=1),ncol=1)
y<-X[,1,drop=FALSE]^2+rnorm(1000,sd=0.1)
# make the outlier
ol<-sample(seq_along(y),66)
y[ol,]<-sample(c(50,100,150,200,-50,-100),1)

# use MSE as the loss function
w_1<-casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=1, eta=0.01)
pred_1<-casl_nn_predict(w_1, X)
# calculate MSE
sum((y-pred_1)^2/1000)

# use MAD as the loss function
w_2<-casl_nn_sgd_mad(X, y, sizes=c(1, 25, 1), epochs=1, eta=0.01)
pred_2<-casl_nn_predict(w_2, X)
# calculate MSE
sum((y-pred_2)^2/1000)
# The mean squared error (MSE) of using MAD as a loss function is larger than using MSE as a loss function. MSE loss function performs better than MAD loss function in terms of robustness.
